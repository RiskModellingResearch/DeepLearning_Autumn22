{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS3tRM8cAFuU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from __future__ import division\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseFeatureLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, nrof_cat, emb_dim, emb_columns, numeric_columns):\n",
        "        super(DenseFeatureLayer, self).__init__()\n",
        "\n",
        "        self.emb_columns = emb_columns\n",
        "        self.numeric_columns = numeric_columns\n",
        "\n",
        "        self.embeddings = nn.ModuleDict()\n",
        "        for i, col in enumerate(self.emb_columns):\n",
        "            self.embeddings[col] = torch.nn.Embedding(nrof_cat[col], emb_dim)\n",
        "\n",
        "        self.feature_bn = torch.nn.BatchNorm1d(input_size)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            # m.bias.data.fill_(0.001)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        numeric_feats = torch.tensor(pd.DataFrame(input_data)[self.numeric_columns].values, dtype=torch.float32)\n",
        "\n",
        "        emb_output = None\n",
        "        for i, col in enumerate(self.emb_columns):\n",
        "            if emb_output is None:\n",
        "                emb_output = self.embeddings[col](torch.tensor(input_data[self.emb_columns[i]], dtype=torch.int64))\n",
        "            else:\n",
        "                emb_output = torch.cat(emb_output, self.embeddings[col](\n",
        "                    torch.tensor(input_data[self.emb_columns[i]], dtype=torch.int64)), dim=1)\n",
        "\n",
        "        concat_input = torch.cat([numeric_feats, emb_output], dim=1)\n",
        "        output = self.feature_bn(concat_input)\n",
        "        return output"
      ],
      "metadata": {
        "id": "viZnwCu0AmQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GLULayer(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(GLULayer, self).__init__()\n",
        "\n",
        "        self.fc = torch.nn.Linear(input_size, output_size)\n",
        "        self.fc_bn = torch.nn.BatchNorm1d(output_size)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            # m.bias.data.fill_(0.001)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = self.fc(input_data)\n",
        "        output = self.fc_bn(output)\n",
        "        output = torch.nn.functional.glu(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "rCP1zsghAGGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, nrof_glu, input_size, output_size):\n",
        "        super(FeatureTransformer, self).__init__()\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5]))\n",
        "        self.nrof_glu = nrof_glu\n",
        "        self.glu_layers = nn.ModuleList([])\n",
        "\n",
        "        for i in range(nrof_glu):\n",
        "            self.glu_layers.append(GLULayer(input_size[i], output_size))\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        layer_input_data = input_data\n",
        "        for i in range(self.nrof_glu):\n",
        "            layer_input_data = torch.add(layer_input_data, self.glu_layers[i](layer_input_data))\n",
        "            layer_input_data = layer_input_data * self.scale"
      ],
      "metadata": {
        "id": "k95o2VcwAGJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentiveTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AttentiveTransformer, self).__init__()\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            # m.bias.data.fill_(0.001)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0Mw7dbFSALkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sparsemax(nn.Module):\n",
        "    \"\"\"Sparsemax function.\"\"\"\n",
        "\n",
        "    def __init__(self, dim=None):\n",
        "        \"\"\"Initialize sparsemax activation\n",
        "        \n",
        "        Args:\n",
        "            dim (int, optional): The dimension over which to apply the sparsemax function.\n",
        "        \"\"\"\n",
        "        super(Sparsemax, self).__init__()\n",
        "\n",
        "        self.dim = -1 if dim is None else dim\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Forward function.\n",
        "        Args:\n",
        "            input (torch.Tensor): Input tensor. First dimension should be the batch size\n",
        "        Returns:\n",
        "            torch.Tensor: [batch_size x number_of_logits] Output tensor\n",
        "        \"\"\"\n",
        "        # Sparsemax currently only handles 2-dim tensors,\n",
        "        # so we reshape to a convenient shape and reshape back after sparsemax\n",
        "        input = input.transpose(0, self.dim)\n",
        "        original_size = input.size()\n",
        "        input = input.reshape(input.size(0), -1)\n",
        "        input = input.transpose(0, 1)\n",
        "        dim = 1\n",
        "\n",
        "        number_of_logits = input.size(dim)\n",
        "\n",
        "        # Translate input by max for numerical stability\n",
        "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
        "\n",
        "        # Sort input in descending order.\n",
        "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
        "        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n",
        "        range = range.expand_as(zs)\n",
        "\n",
        "        # Determine sparsity of projection\n",
        "        bound = 1 + range * zs\n",
        "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
        "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
        "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
        "\n",
        "        # Compute threshold function\n",
        "        zs_sparse = is_gt * zs\n",
        "\n",
        "        # Compute taus\n",
        "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
        "        taus = taus.expand_as(input)\n",
        "\n",
        "        # Sparsemax\n",
        "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
        "\n",
        "        # Reshape back to original shape\n",
        "        output = self.output\n",
        "        output = output.transpose(0, 1)\n",
        "        output = output.reshape(original_size)\n",
        "        output = output.transpose(0, self.dim)\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Backward function.\"\"\"\n",
        "        dim = 1\n",
        "        nonzeros = torch.ne(self.output, 0)\n",
        "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
        "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
        "        return self.grad_input"
      ],
      "metadata": {
        "id": "UIS69nvlAWMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TabNet, self).__init__()\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "            # m.bias.data.fill_(0.001)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "HHqQCwjNAOxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашка\n",
        "1. Дособрать TabNet (https://arxiv.org/pdf/1908.07442.pdf)\n",
        "2. Запустить на датасете income\n",
        "3. Залогировать метрики с помощью tensorboard/mlflow"
      ],
      "metadata": {
        "id": "T4oY0vikJXHP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AS_AwKqqAOzf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}